---
title: "Predicting the Occurrence and Cost of Car Accidents"
author: "Biguzzi, Connin, Greenlee, Moscoe, Sooklall, Telab, and Wright"
date: "11/21/2021"
header-includes:
  - \usepackage{dcolumn}
output: 
  pdf_document: default
  html_document: default
---

```{r setup, include=F}
knitr::opts_chunk$set(echo = F, results = 'asis', comment = NA, warning = F, message = F)
options(knitr.kable.NA = '')
```


# Introduction
This analysis predicts the probability that a given insurance customer will be involved in a car crash, as well as the cost of the crash to the insurance company. We begin with an exploration of the data to build an initial impression on the relationships, which will guide how we transform and select variables. We construct two models: a logistic regression for the binary target variable of Crash vs No Crash; and a least-squares model for the target dollar cost variable.  Ultimately, we will integrate both results to help the insurer evaluate their financial risk.

In this report we will: 

* Explore the data
* Transform data to address multicollinearity and meet variable distribution needs
* Compare different models and select the most accurate model
* Test our model on the evaluation data set

```{r import_packages, include = FALSE}
library(tidyverse)
library(janitor)
library(magrittr)
library(flextable)
library(dlookr)
library(mice)
library(ggpubr)
library(viridis)
library(corrplot)
library(logistf)
library(car)
library(MASS) # step AIC
library(caret)
library(pROC)
library(rcompanion) #CramersV test for categoricals
#library(InformationValue)
library(aod)
library(mgcv)
library(jtools)
library(pscl)
library(broom)
library(gtable)
library(grid)
library(cowplot)
library(gridExtra)
library(grid)
library(ggplot2)
library(lattice)
library(knitr)
library(ggthemes)
```


```{r createfunctions, include = FALSE}
myboxplot <- function(data,x,y){
  fig = data%>%
  ggplot(aes(data[[x]],data[[y]])) +
  geom_boxplot() +
  ggtitle(paste0(y," by ", x))+
    xlab(NULL)+
    ylab(NULL)
print(fig)
}
myhist <- function(data,x,y){
  fig = data%>%
  ggplot(aes(log(data[[y]]) , fill = data[[x]])) +
  geom_histogram() +
  ggtitle(paste0("log ",y," by ", x))+
    xlab(NULL)+
    ylab(NULL)
print(fig)
}
modelmtx  <- function(data, model,name,threshold){ 
target = names(model.frame(model))[1]
data$predicted = factor(ifelse(predict(model,data,type="response") > threshold ,1,0))
confusion <- confusionMatrix(data$predicted, data[[target]])
mresults <- tibble(
       model_name = name,
       predictors = length(coef(model))-1,
       precision = confusion$byClass[[5]],
       auc = auc(roc(response = as.numeric(data[[target]]), predictor = as.numeric(data$predicted)))[1],
       AIC = model$aic, 
       BIC = BIC(model)
       )
    return(mresults)
}
```



```{r import_data, include = FALSE}
evalpath = 'https://raw.githubusercontent.com/mtelab1/CUNYSPS_DATA621/main/HW4/insurance-evaluation-data.csv'
trainpath = 'https://raw.githubusercontent.com/mtelab1/CUNYSPS_DATA621/main/HW4/insurance_training_data.csv'
raw <- read_csv(trainpath)
test <- read_csv(evalpath)
```

#Exploratory Data Analysis and Wrangling

As a first step in our EDA process, we applied a basic set of data cleaning operations at the onset. These included:

1. standardizing column names to remove special characters and convert to lower-case construction.
2. removing any empty rows or columns 
3. updating column types where appropriate 
4. removing special characters and symbols from column values 
5. simplifying select character values for brevity and clarity 
6. converting character column types to factor
7. creating factor levels for the 'education' variable

```{r initial_cleaning, include = FALSE}
# initial clean of col names
raw%<>%clean_names
# remove any empty rows and cols
raw%<>%remove_empty(c("rows", "cols"))
# assess presence of duplicates
get_dupes(raw)
# basic characterization
str(raw)
#clean extraneous symbols from char col values and convert to numeric as appropriate
df<-raw%>%
    mutate_if(is_character, str_replace_all, '\\$|,|z_|<', '')%>%
    mutate_at(c(8,10,17,21), as.numeric)%>%
    mutate_at(c(2), as.factor)
# round numeric columns
df%<>%mutate_if(is.numeric, round)
# identify unique values in our character cols
id_distinct <- df%>%
    dplyr::select(where(is_character))%>%
    map(~str_c(unique(.x),collapse = ",")) %>% 
    bind_rows() %>% 
    gather(key = col_name, value = col_unique)
# Update col values for clarity and brevity
df%<>%
    mutate_if(is_character, str_replace_all, "No|no",'N')%>%
    mutate_if(is_character, str_replace_all, "Yes|yes",'Y')%>%
    mutate_if(is_character, str_replace_all, "Highly Urban/ Urban",'Urban')%>%
    mutate_if(is_character, str_replace_all, "Highly Rural/ Rural",'Rural')
# convert character cols to factor and set level for education col
df %<>%mutate_if(is_character, ~(factor(.)))
df$education<-factor(df$education, levels=c("High School", "Bachelors", "Masters", "PhD"))
```

We then evaluated our data for missing values. Our assessment distinguished 5 numerical variables and 1 categorical variable with missing entries. The proportion of missingness across these variables was relatively low, ranging from ~ .07 to 6.5 percent. 

```{r missingness,fig.height = 8, fig.width = 20}
#basic missing data table
df%>%
    diagnose()%>%
    dplyr::select(-unique_count, -unique_rate)%>%
    filter(missing_count>0)%>%
    arrange(desc(missing_count))%>%
    flextable()
```

We then imputed values for these entries using a combination of recursive partitioning and chained equations. These methods yielded superior results relative to other imputation procedures that we tested (e.g., mode, mean, median, knn). This view is supported by a comparison of the distribution densities for the original data and data with imputed values included for each variable with missing entries (see below). 



```{r imputations, include = FALSE}
#impute numerical vars using dlookr, methods have been preselected based on initial plots.
#car_age
p1 = plot(imputate_na(df, car_age, target_flag, method = "rpart", seed = 999))+labs(x= 'car_age')+
  theme_minimal()+ theme(legend.position = "top")
car_age<-imputate_na(df, car_age, target_flag, method = "rpart", seed = 999)
summary(car_age)
#home_val
p2 = plot(imputate_na(df, home_val, target_flag, method = "rpart"))+
  labs(x= 'home_val')+
  theme_minimal()+
  theme(legend.position = "top")
home_val<-imputate_na(df, home_val, target_flag, method = "rpart")
summary(home_val)
#yoj
p3 = plot(imputate_na(df, yoj, target_flag, method = "rpart"))+
  labs(x= 'yoj')+
  theme_minimal()+
  theme(legend.position = "top")
yoj<- imputate_na(df, yoj, target_flag, method = "rpart")
summary(yoj)
# income
p4 = plot(imputate_na(df, income, target_flag, method = "rpart"))+
  labs(x= 'income')+
  theme_minimal()+
  theme(legend.position = "top")
income<-imputate_na(df, income, target_flag, method = "rpart")
summary(income)
#age
p5 = plot(imputate_na(df, age, method = "rpart"))+  #any of the tests work here
    labs(x= 'age')+
    theme_minimal()+
  theme(legend.position = "top")
age<-imputate_na(df, age, method = "rpart")
#build a working dataframe
temp<-cbind(car_age, home_val, yoj, income, age)
temp%<>%as.data.frame(temp)
df%<>%dplyr::select(!c(car_age, home_val, yoj, income, age))%>%
    cbind(temp)
df%<>%mutate_if(is.numeric, round)
```

```{r, include=FALSE}
#impute values for missing categorical data - job
job<-imputate_na(df, job, method = "mice", seed = 999)
p6 = plot(job)+theme_minimal()+theme(legend.position = "top")
# combine into new df
df<-df%>%dplyr::select(!job)
df<-cbind(df,job)
df$job<-factor(df$job)
```


```{r imputation_plots, fig.height=14, fig.width=8}
#assemble plot grid
g1 <- as_grob(p1)
g2 <- as_grob(p2)
g3 <- as_grob(p3)
g4 <- as_grob(p4)
g5 <- as_grob(p5)
g6 <- as_grob(p6)
grid.arrange(g1, g2, g3, g4 , g5, g6, ncol=1)
```

With a complete data set, we then evaluated our variable distributions relative to a normal model. The histograms and QQ-plots below highlight these distributions and provide alternative outcomes based on transformations (log or square-root) of the data for each variable. We can discern the following:

1. a number of variables are highly right skewed owing to zero inflation: target_amt, kidsdrive, homekids, tif, oldclaim, clm_freq, mvr_pts, car_age, home_val
2. others are right skewed without zero inflation: bluebook, income
3. several numerical variables are also bimodal: car_age, home_val, yoj

A Shapiro-Wilk can also be employed to identify deviations from a normal model. These statistics are included in the following table. We note that a significance level below 0.05 indicates that the data do not fit a normal distribution. 

These assessments provide a basis for identifying potential variable transformations during the modeling stage to improve model fit. And we note that high zero counts may point to contamination in the data that isn't implicit in the variable desriptions. For example, a zero value in home-value may be related to external factors such as renting, under-age driver claims, etc. 


```{r view_distributions , figures-side, fig.show="hold", out.width="50%", include=FALSE}
#identify highly skewed data
df%>%find_skewness(index=FALSE, thres=TRUE)#this is good
# assess normality - Shapiro Wilke
df%>%
    dplyr::select(!index)%>%
    normality()%>%
    mutate(p_value = formatC(p_value, format = "e", digits = 2), statistic = round(statistic,3))%>%
    flextable(theme_fun = theme_booktabs())

```

```{r view_distributions, figures-side, fig.show="hold", out.width="50%"}
df%>%plot_normality()	
```

```{r smalldatacorrection}
#Correct obvious data errors - negative values
df$car_age[df$car_age < 0] <- NA
```

Other diagnostics can be employed to evaluate data structure and ensure data quality. 

The following table provides and overview of the levels of each categorical variable with regard to count, frequency, percentage, and rank. We can draw on this information to guide our modeling descisions downstream. 

```{r}
# other diagnostics

df%>%
    diagnose_category()%>%
    flextable(theme_fun = theme_booktabs())

```

We can view our numerical data in relation to measures of spread and central tendency as well as zero count and negative values. The latter can be used to identify erroneous entries for data that should be non-negative. 

```{r}
df%>%
    diagnose_numeric()%>%
    dplyr::select(variables, min, mean, median, max, zero, minus)%>%
    flextable(theme_fun = theme_booktabs())
```


To complement our review of variable distributions we can also quantify the overall count and proportion of outliers for data within each variable. The presence and overall structure (e.g., clumping) of outliers can point unique circumstances that lend insight the observations. They also provide a basis for identifying data points that may exert high leverage and influence with regard to model fit. 

The following table lists response variable by decreasesing outlier count and related measures. It's clear that outliers comprise more than 10 percent of the data for homekids, kidsdrive, and target_amt. This is also consistent with the high count of zero values for these variables. 


```{r outliers}
diagnose_outlier(df)%>%arrange(desc(outliers_cnt)) %>% 
 mutate_if(is.numeric, round , digits=3)%>% flextable() 
```
We can assess potential changes in our variable distributions in the absence of outliers as shown in the plots below. If we determine that there are legitimate reasons to remove such data due to contamination, data entry mistakes, or influence, we can remove these entries prior to modeling. However, doing so may also result in a loss of information that, in turn,  reduces our ability to construct a model that generalizes well. 
```{r , include= FALSE}
df %>% 
    dplyr::select(find_outliers(df, index = FALSE)) %>% 
    plot_outlier()
```
At this stage, we are well positioned to investigate potential relationships between our response variable (target_flag) and our predictors. For example, the following mosaic plots reveal interactions between our categorical predictors and our response, with the possible exception of two variables, 'sex' and 'red_car'. While we will still include these variables in our initial classification model, it is unlikely that they will be retained during the selection process. 


```{r flag_categorical_relationship, figures-side, fig.show="hold", out.width="50%"}
df %>% 
  target_by(target_flag) %>%      
  relate(parent1) %>% 
  plot()
df %>% 
  target_by(target_flag) %>%      
  relate(mstatus) %>% 
  plot()
df %>% 
  target_by(target_flag) %>%      
  relate(sex) %>% 
  plot()
df %>% 
  target_by(target_flag) %>%      
  relate(education) %>% 
  plot()
df %>% 
  target_by(target_flag) %>%      
  relate(job) %>% 
  plot()
df %>% 
  target_by(target_flag) %>%      
  relate(car_use) %>% 
  plot()
df %>% 
  target_by(target_flag) %>%      
  relate(car_type) %>% 
  plot()
df %>% 
  target_by(target_flag) %>%      
  relate(red_car) %>% 
  plot()
df %>% 
  target_by(target_flag) %>%      
  relate(revoked) %>% 
  plot()
df %>% 
  target_by(target_flag) %>%      
  relate(urbanicity) %>% 
  plot()
```

We can also evaluate interactions between numerical predictors and our response using box-plots. At first approximation, it appears that values for 'travtime', 'age', and 'yoj' are distributed similarly across the two levels of our response; suggesting that they may not have very much predictive value. However, this assessment may be conflated by the presence of outliers, particularly 'travtime'. 


```{r}
num_box<-select_if(df, is.numeric)
num_box<-cbind(df$target_flag, num_box)%>%
    rename(target_flag = 'df$target_flag')
response = names(num_box)[1] #target_flag
response = purrr::set_names(response)
explain <- names(num_box)[3:16] #explanatory variables
explain = purrr::set_names(explain)
box_fun = function(x) {
    ggplot(num_box, aes_string(x = x, y = 'target_flag') ) +
    geom_boxplot(aes(fill = target_flag, alpha = 0.4), outlier.color =
    'red', show.legend = FALSE)+
    scale_fill_viridis(discrete = TRUE, option = "E")+
    coord_flip()+
    theme_classic()
    
}
b_plots<-map(explain, ~box_fun(.x)) #creates a list of plots
ggarrange(plotlist=b_plots, height = .5, ncol = 3)
```
As an additional check, we can look at the relative distribution of our variables in each response level using histograms (below). It's interesting to note the overall similarity in shape for these subsets with each variable. This pattern may be inherent to the data or, more likely, an indication that the data has been manufactured. 


```{r flag_numerical_relationship, message=FALSE}
#Distributions with target_flag values used for fill. 
for (v in seq(1,length(df))) {
      skip_to_next <- FALSE
      tryCatch(
        myhist(df,'target_flag', df_num[v]
              )
      ,error=function(e){skip_to_next <<- TRUE})
      if(skip_to_next) { next }  
}
```

We should also document any substantive correlations (collinearity) between our covariates that, if unaccounted for, might lead to model misspecification. A paired correlation coefficient greater than .90 would certainly be cause for concern. We see from the correlation matrix below that 'home_val' and 'income' display the greatest correlation (~ 0.58) followed by "old_clm' and clm_freq (~0.50). 

It appears that collinearity will not be a primary concern in our modeling process. Nonetheless, it may be prudent to revisit these higher correlations as we refine our model inputs - to account for interactions, etc.  



# !!!! Not sure what this line of code is for.

```{r preserve_df}
dfpreserve = df
```

# !!!! Given our correlation matrix, I dont think we need to include the following table. 

```{r covariance, include= FALSE}
#assess covariance 

df%>%
    
    correlate()%>%
    filter(coef_corr > .5 | coef_corr < -.5)%>%
    flextable()
```



```{r plot covariance, fig.height=8, fig.width=8, fig.align = 'center'}
df%>%plot_correlate() 
```


\newpage



# Construct Logistical Classification Model

To complete our preprocessing steps, we assessed class imbalance in our response variable, finalized modifications to our covariates, and split our data into training and test sets. 

As seen in the table below, there is a 3:1 class imbalance in our response variable. However, this discrepancy is not extreme and does not require corrective steps - as might be the case with a rare event. That said, we did employ options for rebalancing the data after creating a training set. The former included under- and oversampling as well as class weights. These strategies yielded such poor results (i.e., inflated model AIC values) that we proceeded to model with unbalanced data. 

```{r}
df%>%count(target_flag)%>%
    mutate(frequency = n/sum(n))%>%
    flextable()%>%
  set_caption('All Observations')
```
As noted, we did make several additional modifications to select covariates prior to building our first classification model. These included:

1. converting 'kidsdrive' to a two-level factor (Y/N) -- to reduce the effect of zero inflation
2. condensing 'job' into two levels, 'blue collar' and 'professional' --  to test preconceived ideas that automobile accidents and claims differentiate across these socioeconomic classes.  
3. factoring and leveling the 'education' variable to reflect recognized attainment outcomes. 


```{r set_flagdf}
df%<>%
    dplyr::select(!c(target_amt))  # remove target_amt
# change kidsdrive to categorical
df%<>%
    mutate(kidsdriv = case_when(kidsdriv == 0 ~ 'N'
         ,TRUE  ~ 'Y'))
# change job into blue collar and professional levels
df%<>%
    mutate(job = case_when(job == 'Blue Collar' ~ 'Blue Collar',
                           job != 'Blue Collar' ~ 'Professional', 
                           TRUE ~ as.character(NA)))
#change chars to factors and level education
df %<>%mutate_if(is_character, ~(factor(.)))
df$education<-factor(df$education, levels=c("High School", "Bachelors", "Masters", "PhD"))
```

We then partitioned our data set into training and testing sets. Each split with a 3:1 ratio of negative (0) and positive (1) observations for our response variable.

```{r}
#using caret
 set.seed(6848) #randomization`
    
 #creating indices
 
index <- createDataPartition(df$target_flag,p=0.75,list=FALSE)
    
 #splitting data into training/testing data using the trainIndex object

df_train <- df[index,] #training data (75% of data)  -- note we have one removed row for car age. Need to omit due to NA in train set. 
df_train <- na.omit(df_train)
    
df_test <- df[-index,] #testing data (25% of data)

#check counts and frequencies of target_flag for train set and test set - helpful when comparing to confusion matrix

df_train%>%count(target_flag)%>%
    mutate(frequency = n/sum(n))%>%
    flextable()%>%
  set_caption('Training Sample')

df_test%>%count(target_flag)%>%
    mutate(frequency = n/sum(n))%>%
    flextable()%>%
  set_caption('Test Sample')

```

## Model 1: Base logistic model

Our initial classification model included the full set of predictors. And use of the Akaike information criterion (AIC) to automate variable selection. 

Comparing our model's residual deviance (5499) and degrees of freedom (6094) provides one measure of model fit. On this basis, it does appear that our model performs well relative to a null expectation. This is further confirmed by our Pearson Chi Square statistic which is significant at 0.05. 

After variable selection and final elimination of nonsignifiant variables, our model has the following form:

target_flag ~ kidsdriv + parent1 + mstatus + education + travtime + car_use + bluebook + tif + car_type + oldclaim + clm_freq + revoked + mvr_pts + urbanicity + home_val + income + job

```{r set_flagm1}
#build model
df_train%<>%
    dplyr::select(!index)
model1 <- glm(target_flag ~ ., df_train, family='binomial')%>%
    stepAIC(trace=0) # use Akiaike step, trace 0 prevents intermediate printing, rename model to preserve base
summary(model1)
```

#### Model 1 Evaluation

Review of our model diagnostics indicates that the model performs well (AUC = ~ .81) in discriminating between our negative (1=crash) and positive (o= no crash) class outcomes using an evaluation threshold of 0.50. This is consistent with our model sensitivity (i.e., true positive rate = 0.92) and to a lesser extent, specificity (true negative rate = 0.41) measures. 

***Diagnostics***
```{r, include= FALSE}

#look at fit metrics - create parallel df for this purpose

model1_df<-df_train
model1_df$predicted<-predict(model1,model1_df,type='response')
model1_df%<>%
  mutate(predicted_obs = case_when(
        predicted >= 0.5 ~ 1,
        predicted < 0.5 ~ 0))
model1_df$predicted_obs<-as.factor(model1_df$predicted_obs)
model1_df$target_flag<-as.factor(model1_df$target_flag)

#create confusion matrix
(model1_cm<-confusionMatrix(data = model1_df$predicted_obs, reference = model1_df$target_flag))

# related model results - drawing code from HW3
(model1_metrics <- tibble(model = "Base Model: base variables",
                  predictors = length(coef(model1))-1,
                  sensitivity = model1_cm$byClass[1],
                  specificity = model1_cm$byClass[2],
                  pos_rate = model1_cm$byClass[3],
                  neg_rate = model1_cm$byClass[4],
                  precision = model1_cm$byClass[5],
                  recall = model1_cm$byClass[6],
                  f1=model1_cm$byClass[7],
                  auc = auc(roc(response = as.numeric(model1_df$target_flag),
                                predictor = as.numeric(model1_df$predicted)))[1],  #note: using predicted (probs) vs predicted_obs (0,1)
                  AIC = model1$aic, BIC = BIC(model1)))
```


\newpage




```{r flagm1_diagnostics, fig.align= 'center'}
#roc curve with AUC
 
par(pty='s')
proc<- roc(response=model1_df$target_flag, predictor=model1_df$predicted, plot=TRUE, legacy.axes=TRUE, auc.polygon=TRUE, col='blue', main = 'Model 1 ROC Curve', max.auc.polygon=TRUE, print.auc=TRUE)
```

***Dispersion***



We did not detect overdispersion in the model1 based on our assessment of the residual deviance and chi-square test.

```{r flagm1_dispersion}
# evaluate using deviance and quasibinomial comparison
devres = round(deviance(model1)/df.residual(model1),4) # if considerably greater than one we should be concerned
print(paste0('We divide the deviance by the residuals to obtain the value ', devres,'. There is no overt concern since the values is not greater than 1'))
# dble check with two model fit
quasi_model <-  glm(target_flag ~ .,family='quasibinomial', df_train) # note: using df_train
devchi = pchisq(summary(quasi_model)$dispersion * model1$df.residual,
model1$df.residual, lower = F)
devchi = round(devchi,4)
print(paste0('Next we obtain a Pearson Chi-Squared test statistic of ', devchi ,  ' This communicates that the null hypothesis is not rejected and their are no problems with dispersion.'))
```

***Assumption of Linearity***

A key assumption in a logistic model is that the logit and covariates are linearly related. Our tests for linearity confirm the validity of our model with the possible exception of 'home-kids'. We note, however, that evidence for nonlinearity is not clear in this case.


```{r flagm1_linearity}
#incorporate logit into model1_df
model1_df%<>%
    mutate(logit = log(predicted/(1-predicted)))
# check linearity btwn numerical predictors and logit
with(model1_df, scatter.smooth(travtime, logit, lpars = list(col = "red", lwd = 3, lty = 3)))
with(model1_df, scatter.smooth(tif, logit, lpars = list(col = "red", lwd = 3, lty = 3)))
with(model1_df, scatter.smooth(mvr_pts, logit, lpars = list(col = "red", lwd = 3, lty = 3)))
with(model1_df, scatter.smooth(home_val, logit, lpars = list(col = "red", lwd = 3, lty = 3)))
with(model1_df, scatter.smooth(homekids, logit, lpars = list(col = "red", lwd = 3, lty = 3)))
with(model1_df, scatter.smooth(clm_freq, logit, lpars = list(col = "red", lwd = 3, lty = 3)))
with(model1_df, scatter.smooth(bluebook, logit, lpars = list(col = "red", lwd = 3, lty = 3)))
```

***Outliers & Influential Points***

To assess any impact of influential observations on our model results we examined both the standardized residuals (.std.resid) and the Cookâ€™s distance (.cooksd). While the latter distinguished several notable outliers (3722, 3592, 6501), these data were not influential (D >1). This is consistent with our plot of standardized residuals (below) - with no observations exeeding 3 standard deviations. 


**http://www.sthda.com/english/articles/36-classification-methods-essentials/148-logistic-regression-assumptions-and-diagnostics-in-r/

```{r flagm1_Outliers_Influenctial , include= FALSE}
# Extract model results
model1$data <- augment(model1) %>% 
  mutate(index = 1:n()) 
#top 10 largest values
model1$data %>% top_n(10, .cooksd)
#Filter potential influential data points with abs(.std.res) > 3
model1$data %>% 
  filter(abs(.std.resid) > 3)
```
```{r}
plot(model1, which = 4, id.n = 3)  # keep an eye on obs > 4/n 
#plot std residuals
ggplot(model1$data, aes(index, .std.resid)) + 
   geom_point(aes(color = target_flag), alpha = .5) +
  theme_bw()
```

***Check for Independence***

An additional assumption in a logistic regression model is independence among model residuals. This can be assessed by plotting binned residuals against the linear predictor (logit). On this basis, there does appear to be a pattern of dependence among our residuals (below) which points to a model misspecification. We speculate that this may derive from grouping effects among our categorical variables, given that we are not using time series data. Despite extensive experimentation, we were not able to remove this effect from this model or others that follow.  


```{r flagm1_Independence, fig.height=8, fig.width=8}
res_chk<-model1_df%>%mutate('residuals' = residuals(model1), linpred = predict(model1))
bins<- group_by(res_chk, cut(linpred, breaks=unique(quantile(linpred, (1:100)/101))))
diag<-summarize(bins, residuals=mean(residuals), linpred = mean(linpred))
plot(residuals~linpred, diag, xlab = "linear predictor")
```

***Goodness of Fit - marginal plots***

As a final check on our fitted model, we plotted the marginal effects for each of our variables (below). Our results indicated that variable transformations may help improve the fit for travtime, bluebook, income, and possible, clm_freq. 

```{r flagm1_fit, fig.align='center', fig.height=10,fig.width=10, message = FALSE}
marginals<-mmps(model1,main=NULL)
```
\newpage

## Model 2: Apply Predictor Transformations

Our second classification model, model2, included transformations on 'income' (square root), 'bluebook' (log), and travtime (quadratic). We decided on these transformations on the basis of their distributions as well as iterative experimentation with the model. Similar to model1, we employed AIC to assist variable selection. Model2 has the following form after selection:

 target_flag = kidsdriv + parent1 + mstatus + education + car_use + tif + car_type + oldclaim + revoked + urbanicity + home_val + job + travtime + I(travtime^2) + mvr_pts + clm_freq + log_bluebook + sqrt_income
 
Our model AIC (5530) decreased slightly relative to model1 (5551) indicating a slightly better fit to the data. This is also indicated by the slight increase in our model's AUC (.831). Other measure of performance were similar to model1 (sensitivity = .92, specificity = .42).

The transformations applied to 'income', 'bluebook', and 'travtime' did improve the marginal effects of our model, as inferred from our marginal plots (below).


```{r set_flagdf2 , message = FALSE}
#create df to contain transformed sqrt, log, vars 
trans_df<-df_train
trans_df%<>%
    mutate(log_bluebook= log(bluebook))%>%
    mutate(sqrt_income=sqrt(income))%>%
    dplyr::select(!c(bluebook, income))
```


```{r set_flagm2 ,message = FALSE}
#build model with transformed vars
# remove yoj and homekids since they were not significant at .05
model2 <- glm(target_flag~kidsdriv+parent1+mstatus+education+car_use+tif+car_type+oldclaim+revoked+urbanicity+home_val+job+travtime+I(travtime^2)+
                  mvr_pts+clm_freq+log_bluebook+sqrt_income,family='binomial',trans_df)%>%stepAIC(trace=0) 
summary(model2)
```
\newpage

#### Model 2 Evaluation

***Diagnostics***

```{r flagm2_diagnostics , fig.align='center', fig.height=10,fig.width=10, message = FALSE}
#additional metrics
trans_df$predicted<-predict(model2, trans_df, type='response')
trans_df%<>%
  mutate(predicted_obs = case_when(
        predicted >= 0.5 ~ 1,
        predicted < 0.5 ~ 0))
trans_df$predicted_obs<-as.factor(trans_df$predicted_obs)
trans_df$target_flag<-as.factor(trans_df$target_flag)
#confusion matrix
(model2_cm<-confusionMatrix(data = trans_df$predicted_obs, reference = trans_df$target_flag))
# transformation results
(model2_metrics <- tibble(model = "transformation Model: reduced variables",
                  predictors = length(coef(model2))-1,
                  sensitivity = model2_cm$byClass[1],
                  specificity = model2_cm$byClass[2],
                  pos_rate = model2_cm$byClass[3],
                  neg_rate = model2_cm$byClass[4],
                  precision = model2_cm$byClass[5],
                  recall = model2_cm$byClass[6],
                  f1=model2_cm$byClass[7],
                  auc = auc(roc(response = as.numeric(trans_df$target_flag),
                                predictor = as.numeric(trans_df$predicted)))[1],
                  AIC = model2$aic, BIC = BIC(model2)))
#roc curve with AUC
par(pty='s')
proc<- roc(response=trans_df$target_flag, predictor=trans_df$predicted, plot=TRUE, legacy.axes=TRUE, auc.polygon=TRUE, col='blue', main = 'PROC ROC Curve', max.auc.polygon=TRUE, print.auc=TRUE)
#review marginal plots
model2_marg<-mmps(model2,main=NULL)
```

\newpage

***Independence***

The persistence of a pattern means that we cannot yet rule out the possibility that the model is misspecified.

```{r fig.height=8, fig.width=8}
res_chk<-trans_df%>%mutate('residuals' = residuals(model2), linpred = predict(model2))
bins<- group_by(res_chk, cut(linpred, breaks=unique(quantile(linpred, (1:100)/101))))
diag<-summarize(bins, residuals=mean(residuals), linpred = mean(linpred))
plot(residuals~linpred, diag, xlab = "linear predictor")
```
\newpage

## Model 3 - Feature engineering and Interactions among predictor variables

For our final model, model3, we established a new variable called 'liquidity' which comprised the a ratio of home value to income - with 1 added to numerator and denomenator to offset the effect of zero values. We established this variable to account for correlation between these variables (see below) and to address possible contamination in the data. For example the excess of zero values for 'home_val' and 'income' could owe to the presence of renters, under-age dependents, and/or unemployed drivers. 

To better capture grouping effects (see histograms) and to simplify our model we constructed the following factor types from selected numerical variables. 

-`mvr_pts` = factor("none", "low", "high")
-`investment` = factor("low", "high")
-`tif` factor=("low", "moderate", "high")
-`clm_freq` = factor("none","moderate","high")

We also tested for possible interactions between categorical variables (CramersV) and continuous and categorical variables (logistic regression). Our results indicated possible interactions between 'car_use' and 'car_type', 'urbanicity' and 'travtime', 'mvr_pts' and 'revoked', and 'car_type' and 'clm_freq'. We included these interactions in model3. 

```{r include= FALSE}
#create dataframe for evaluating interactions
int_df<- df_train
#check for correlations between continuous variables
num_data<-int_df%>%
    dplyr::select(where(is_numeric))
num_data%>%
    correlate()%>%
    filter(coef_corr > .5 | coef_corr < -.5)%>%
    flextable() # home data and income have .56 corr
```


```{r set_flagdf3}
int_df%>%
    ggplot(aes(income, home_val, color=target_flag))+
    geom_point()+
    theme_classic()
```
```{r, include=FALSE}
# with zero home_val removed income and home_val correlate at .91
filtered_data<-int_df%>%
    filter(income > 0 & home_val > 0)%>%
    dplyr::select(c(income, home_val))%>%
    correlate()
#Check for correlation between categorical variables
cat<-int_df%>%
    dplyr::select(where(is.factor))
    
cramerV(cat$education, cat$job)  #somewhat weak association
cramerV(cat$car_use, cat$car_type)   # relatively strong association
cramerV(cat$kidsdriv, cat$mstatus) # no assoc
#Check for correlation between select categorical and continuous variables
urb_trav<-glm(urbanicity~travtime, int_df, family=binomial) # significant
rev_mvr<-glm(revoked~mvr_pts, int_df, family=binomial) # significant
kid_clm<-glm(kidsdriv~clm_freq, int_df, family=binomial) # not significant
car_clm<-glm(car_type~clm_freq, int_df, family=binomial) # significant
summary(urb_trav)
summary(rev_mvr)
summary(kid_clm)
summary(car_clm)
```




```{r set_flagdf3vars,include=FALSE}
# Evaluate liquidity via. histogram -- we find two distinct groups that can be converted to factor variable (liquidity) with low/high values 
int_df%>%
    mutate(home_val = home_val+1)%>%
    mutate(income = income +1)%>%
    mutate(liquidity = home_val/income)%>%
    ggplot(aes(liquidity))+
    geom_histogram()+
    theme_classic()
int_df%<>%
    mutate(home_val = home_val+1)%>%
    mutate(income = income +1)%>%
    mutate(liquidity = home_val/income)%>%
    mutate(liquidity = cut(liquidity, breaks = c(-Inf, 1, Inf), labels=c("low", "high")))%>%
    dplyr::select(!c(home_val, income))
               
#convert mvr_pts to factor
int_df%<>%
    mutate(mvr_pts = cut(mvr_pts, breaks = c(-Inf, 0, 3, Inf), labels=c("none", "low", "high")))
#convert tif to factor
                         
int_df%<>%
    mutate(tif = cut(tif, breaks = c(0, 5, 10, Inf), labels=c("low", "moderate", "high")))   
#convert clm_freq to factor
                   
int_df %<>% mutate(clm_freq=cut(clm_freq, breaks=c(-Inf, 0, 3, Inf), labels=c("none","moderate","high")))
```

Model3 had the following form after variable selection with AiC

target_flag = kidsdriv + parent1 + mstatus + education + travtime + car_use + log(bluebook) + tif + car_type + 
    oldclaim + clm_freq + revoked + mvr_pts + urbanicity + liquidity + car_use:car_type + travtime:urbanicity
    
Our model AIC (5586) incrased relative to model2 and model1 indicating a slightly poorer fit to the data. This is also indicated by the slight decrease in our model3's AUC (.81). Other measure of performance were similar to model1 (sensitivity = .92, specificity = .40).
    
Other model diagnostics (e.g., linearity, dispersion, influential obs, residual independence) yielded resuts similar to model1 and model2.

```{r set_flagm3}
model3<- glm(target_flag~ kidsdriv + parent1 + mstatus + education + travtime +  car_use + I(log(bluebook)) + tif + car_type + oldclaim + clm_freq+revoked + mvr_pts + urbanicity + liquidity + job + car_use*car_type+urbanicity*travtime+mvr_pts*revoked++car_type*clm_freq, int_df, family= binomial)%>%stepAIC(trace=0)

summ(model3)
```

\newpage

***Diagnostics***

```{r flagm3_diagnostics}
#additional metrics
int_df$predicted<-predict(model3, int_df, type='response')
int_df%<>%
  mutate(predicted_obs = case_when(
        predicted >= 0.5 ~ 1,
        predicted < 0.5 ~ 0))
int_df$predicted_obs<-as.factor(int_df$predicted_obs)
int_df$target_flag<-as.factor(int_df$target_flag)
#confusion matrix
model3_cm<-confusionMatrix(data = int_df$predicted_obs, reference = int_df$target_flag)
# transformation results
(model3_metrics <- tibble(model = "Feature_Eng+Transform Model: reduced variables",
                  predictors = length(coef(model3))-1,
                  sensitivity = model3_cm$byClass[1],
                  specificity = model3_cm$byClass[2],
                  pos_rate = model3_cm$byClass[3],
                  neg_rate = model3_cm$byClass[4],
                  precision = model3_cm$byClass[5],
                  recall = model3_cm$byClass[6],
                  f1=model3_cm$byClass[7],
                  auc = auc(roc(response = as.numeric(int_df$target_flag),
                                predictor = as.numeric(int_df$predicted)))[1],
                  AIC = model3$aic, BIC = BIC(model3)))
```
```{r flagm3_diagnostics , fig.align='center', fig.height=5,fig.width=5, message = FALSE}
#roc curve with AUC
par(pty='s')
proc<- roc(response=int_df$target_flag, predictor=int_df$predicted, plot=TRUE, legacy.axes=TRUE, auc.polygon=TRUE, col='blue', main = 'PROC ROC Curve', max.auc.polygon=TRUE, print.auc=TRUE)
#review marginal plots
model3_marg<-mmps(model3,main=NULL)
```


```{r flagm3_dispersion}
# evaluate using deviance and quasibinomial comparison
devres = deviance(model3)/df.residual(model3) # if considerably greater than one we should be concerned
# dble check with two model fit
quasi_model <-  glm(target_flag ~ .,family='quasibinomial', int_df) # note: using df_train
devchi = pchisq(summary(quasi_model)$dispersion * model3$df.residual,
model3$df.residual, lower = F)  
devchi = round(devchi,4)
print(paste0('We divide the deviance by the residuals to obtain the value ', devres,'. There is no overt concern since the values is not greater than 1'))
print(paste0('Next we obtain a Pearson Chi-Squared test statistic of ', devchi ,  ' This communicates that the null hypothesis is not rejected and their are no problems with dispersion.'))
```

```{r fig.height=8, fig.width=8}
res_chk<-int_df%>%mutate('residuals' = residuals(model3), linpred = predict(model3))
bins<- group_by(res_chk, cut(linpred, breaks=unique(quantile(linpred, (1:100)/101))))
diag<-summarize(bins, residuals=mean(residuals), linpred = mean(linpred))
plot(residuals~linpred, diag, xlab = "linear predictor")
```
\newpage

## Logistic Classification Model Selection

Model performance is similar across all cases. Model1 had the highest accuracy. Model2 has the lowest AIC.

The models are able to discriminate response classes effectively given a 0.5 evaluation threshold. While the discriminatory performance was similar across our models, we selected model2 for test validation since it had the lowest AIC and highest AUC values

```{r}
options(width = 40)
(metrics<-rbind(model1_metrics, model2_metrics, model3_metrics)%>%flextable())
```

\newpage

# Construct Linear Regression Model

The full model includes only four variables with significant p-values, and the r-squared is very low.  Also the residual plots fail the required assumptions regarding the normal distribution and constant variance. We will experiment with variable selection, but we also need to either transform the response variable or change the link function.

```{r train_arg, include= FALSE}
train_control <- trainControl(method = "cv",number = 10, p =.2, predictionBounds = c(0,NA))
```

```{r}
detach("package:MASS", unload = TRUE)
dfcrash = dfpreserve%>%
  filter(target_flag==1)%>%
  select(-'index',-'target_flag')%>%
  na.omit()
```



### Model 1: Base Linear Model

```{r cost_model_saturated}
costm1 = lm(target_amt ~., data = dfcrash)
summ(costm1)
plot(costm1)
```


## Cost Model 2: Feature Reduction


Removing the variables below reduces Residual Standard Error:  -`parent1`  
-`age`  
-`homekids`  
-`kidsdriv`  
-`red_car`  
-`urbanicity`  
-`job`  
```{r cost_model_1}
dfcrashm2 = dfcrash %>%
  select(-parent1, -age, -homekids, - kidsdriv, -red_car, -urbanicity, -job, - red_car)
costm2 = lm(target_amt ~ .,dfcrashm2)
summ(costm2)
plot(costm2)
#costm1 <- step(costm1 ,direction = 'forward',trace=0)
#model1cv <- train(target_amt ~., data = dfcrashm1, 
      #         method = "lm",
       #        trControl = train_control)
```

\newpage

## Cost Model 3: Transformation & Weights
We attempt to correct the heteroskedasticity of the residual plots through transformations.

Lets review the linearity from the plots below.

```{r, fig.height=7}
df_num = dfcrash %>% select(where(is.numeric),-target_amt) %>% names()
dfcrash %>%
  gather(df_num, key = "var", value = "value") %>%
  ggplot(aes(x = value, y = target_amt)) +
  geom_point() +
  facet_wrap(~ var, scales = "free") +
  ggthemes::theme_fivethirtyeight()
```

Now with a log transformation on the response.  We can see that this removes much of the linearity we noticed above.

```{r, fig.height=7}
df_num = dfcrash %>% select(where(is.numeric),-target_amt) %>% names()
dfcrash %>%
  gather(df_num, key = "var", value = "value") %>%
  ggplot(aes(x = value, y = log(target_amt))) +
  geom_point() +
  facet_wrap(~ var, scales = "free") +
  ggthemes::theme_fivethirtyeight()
```

Below is another set of plots that feature the below predictor transformations.

```{r, fig.height=7}
df_num = dfcrash %>% select(where(is.numeric),-target_amt) %>% names()
dfcrashm3 = dfcrash %>%
  mutate(income = sqrt(income),bluebook = log(bluebook) ,travtime = travtime**2)
dfcrashm3%>%
  gather(df_num, key = "var", value = "value") %>%
  ggplot(aes(x = value, y = target_amt))+
  geom_point() +
  facet_wrap(~ var, scales = "free") +
  ggthemes::theme_fivethirtyeight()
```

The last component to our third model is the application of weights.  The first step shown below is the calculation of these weight coefficients. Our strategy is to use the results from our base model, regressing the residuals against its fitted values.  We end up with a distribution of values which loosely represent the variance.  By taking the absolute value of this regression we can place less value on the observations with greater variance.

Below is a plot of how the weights along the scale of the response variable.
```{r set_wieghts, include=FALSE}
weightlm = glm(costm1$residuals ~ costm1$fitted.values)
wts = weightlm$fitted.values+abs(min(weightlm$fitted.values))
#+ abs(min(weightlm$fitted.values))
```
```{r, fig.align = 'center'}
plot(costm1$fitted.values, wts)
```

The below weighted model has the lowest residual standard error but the R^2 is still very low.
```{r cost_model_3}
costm3 = lm(target_amt ~ .,dfcrashm3,weights = wts )
summary(costm3)
plot(costm3)
# summary(costm3)
# plot(costm3)
#costm3cv <- train(target_amt ~.,data = dfcrashm3,method = "lm", weights = wts,trControl = train_control)
#summary(costm3cv)
#plot(costm3cv$finalModel)
```

As an alternative approach to weighting the model we implement a Robust Linear Regression. The function uses Iteratively Reweighted Least Squares (IRLS) to obtain a maximum likelihood estimate of the parameters. The residual standard error is reduced again.

```{r cost_model_4}
library(MASS)
costm3b = rlm(target_amt ~ .,dfcrashm3, 
             method = "MM",
              weights = wts
              )
summary(costm3b)
```
```{r}
plot(costm3b$fitted.values,dfcrashm3$target_amt)
```


\newpage

## Cost Model 5 Target Interaction Term

Although there has been some improvement across the above models, $R^2$ is still very low.  We now move to rethink the target variable.  It stands to reason that the cost of a crash is mostly a function of the value of the car, and the p-values from the above models support this hypothesis. Rather than regressing on the cost, which renders most predictors useless, we model the severity of the accident. We can represent severity as `cost`/`bluebook`.

We will drop the ratios that are >1 assuming these involved incidental bodily harm.
```{r}
detach("package:MASS", unload = TRUE)
dfcrashm5 = dfcrash%>% 
  mutate(scale = pmin(1,target_amt/bluebook) 
      #   , income = sqrt(income) ,travtime = travtime**2
         )%>%
  select(-target_amt)%>%
  filter(scale<1)
```

Now lets take another look at the relationships.
```{r, fig.height=9}
df_num = dfcrashm5  %>% select(where(is.numeric)) %>% names()
#dfcrashm5  = dfcrashm5 %>%
#  mutate(income = sqrt(income),bluebook = log(bluebook) ,travtime = travtime**2)
# dfcrashm5 %>%
#   gather(df_num, key = "var", value = "value") %>%
#   ggplot(aes(x = value, y = dfcrashm5$scale))+
#   geom_point() +
#   facet_wrap(~ var, scales = "free") +
#   ggthemes::theme_fivethirtyeight()
```


```{r set_wieghts_, include=FALSE}
samplelm = lm(scale ~ ., dfcrashm5)
weightlm = glm(samplelm$residuals ~ samplelm$fitted.values)
wts = (weightlm$fitted.values) +abs(min(weightlm$fitted.values))
#+ abs(min(weightlm$fitted.values))
plot(samplelm$fitted.values, wts)
```

```{r cost_model_5__}
costm5 = lm(scale ~ ., dfcrashm5)
summ(costm5)
plot(costm5)
```

```{r cost_model_5}
costm5b <- gam(scale ~ age + sex + mstatus + tif + red_car + car_age + home_val  + parent1 + mstatus + education + 
    car_use + tif + car_type + oldclaim + revoked + urbanicity + 
    home_val + job + travtime + bluebook + mvr_pts + clm_freq + 
     income , wieghts = wts,family=betar(link="logit"), data=dfcrashm5)
summary(costm5b)
plot(predict.gam(costm5b,dfcrash,type="response")*dfcrash$bluebook,dfcrash$target_amt)
```





```{r kfold}
# costm5cv <- train(sqrt(scale) ~.,data = dfcrash5,method = "lm",trControl = train_control)
# print(costm5cv)
# plot(costm5cv$finalModel)
```



```{r}
# test = test %>%
#   clean_names %>%
#   
# flagwinner = model3
# costwinner = costm5b
# 
# # 
# test$target_flag = predict(flagwinner,test) 
# test$target_amt = predict(costwinner,test) 
# test$expected_cost = test$target_flag * test$target_amt 
``` 